"""
Hallucination detection module for RAG systems.
"""

import logging
import re
from typing import Dict, List, Optional, Any, Union, Tuple
import numpy as np
from collections import Counter
import json
import os

logger = logging.getLogger(__name__)


class HallucinationDetector:
    """
    Detects and quantifies hallucinations in LLM-generated content.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Initialize the hallucination detector.

        Args:
            config: Configuration options for hallucination detection
        """
        self.config = config or {
            "detection_method": "combined",  # Options: semantic, factual, combined
            "confidence_threshold": 0.7,
            "semantic_similarity_model": "sentence-transformers/all-mpnet-base-v2",
            "use_llm_verification": True,
            "max_chunk_size": 1000,  # Maximum chunk size for processing
        }
        
        logger.info("Hallucination detector initialized with config: %s", self.config)
        
        # Initialize embedding model for semantic comparison if available
        self.embedding_model = self._initialize_embedding_model()

    def _initialize_embedding_model(self):
        """Initialize embedding model for semantic similarity checks."""
        try:
            from sentence_transformers import SentenceTransformer
            
            model_name = self.config.get("semantic_similarity_model", 
                                        "sentence-transformers/all-mpnet-base-v2")
            
            logger.info(f"Loading semantic embedding model: {model_name}")
            return SentenceTransformer(model_name)
            
        except ImportError:
            logger.warning("Sentence-transformers package not found. Semantic hallucination detection will be limited.")
            return None
        except Exception as e:
            logger.error(f"Error loading embedding model: {str(e)}")
            return None

    def detect_hallucinations(
        self, 
        generated_text: str, 
        source_documents: List[Dict[str, Any]], 
        query: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Detect hallucinations in generated text by comparing to source documents.

        Args:
            generated_text: The text generated by the LLM
            source_documents: List of source documents used for generation
            query: Optional original query for context

        Returns:
            Dictionary with hallucination metrics and details
        """
        detection_method = self.config.get("detection_method", "combined")
        
        # Start with initial metrics
        results = {
            "hallucination_score": 0.0,
            "hallucination_types": [],
            "unsupported_claims": [],
            "citation_accuracy": 100.0,
            "factual_consistency_score": 0.0,
            "semantic_consistency_score": 0.0,
            "confidence": 0.0,
            "has_hallucination": False
        }
        
        # Use all available detection methods based on configuration
        if detection_method in ["semantic", "combined"]:
            semantic_results = self._semantic_hallucination_detection(
                generated_text, source_documents, query)
            results.update(semantic_results)
            
        if detection_method in ["factual", "combined"]:
            factual_results = self._factual_hallucination_detection(
                generated_text, source_documents, query)
            results.update(factual_results)
            
        if self.config.get("use_llm_verification", True):
            verification_results = self._llm_based_verification(
                generated_text, source_documents, query)
            
            # If LLM verification is used, combine the results
            results["hallucination_score"] = (
                results["hallucination_score"] + verification_results["hallucination_score"]
            ) / 2
            
            # Combine unsupported claims
            results["unsupported_claims"].extend(verification_results["unsupported_claims"])
            results["unsupported_claims"] = list(set(results["unsupported_claims"]))
            
        # Set final confidence and make determination
        threshold = self.config.get("confidence_threshold", 0.7)
        results["has_hallucination"] = results["hallucination_score"] > threshold
        results["confidence"] = 1.0 - abs(results["hallucination_score"] - 0.5) / 0.5
        
        return results

    def _semantic_hallucination_detection(
        self, 
        generated_text: str, 
        source_documents: List[Dict[str, Any]], 
        query: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Detect hallucinations using semantic similarity between generated text and sources.
        
        Args:
            generated_text: The text generated by the LLM
            source_documents: List of source documents used for generation
            query: Optional original query for context
            
        Returns:
            Dictionary with semantic hallucination metrics
        """
        if not self.embedding_model:
            return {
                "semantic_consistency_score": 0.5,  # Neutral score
                "hallucination_score": 0.5,  # Neutral score
            }
            
        try:
            # Extract sentences from generated text
            sentences = self._extract_sentences(generated_text)
            
            # Skip very short/simple sentences which are less likely to contain hallucinations
            filtered_sentences = [s for s in sentences if len(s.split()) > 3]
            
            if not filtered_sentences:
                return {
                    "semantic_consistency_score": 1.0,  # Perfect score if no substantial sentences
                    "hallucination_score": 0.0,
                }
            
            # Create source text by joining all source documents
            source_text = " ".join([doc.get("text", "") for doc in source_documents])
            source_chunks = self._chunk_text(source_text, self.config.get("max_chunk_size", 1000))
            
            # Get embeddings
            sentence_embeddings = self.embedding_model.encode(filtered_sentences)
            source_chunk_embeddings = self.embedding_model.encode(source_chunks)
            
            # Calculate max similarity for each sentence
            max_similarities = []
            unsupported_claims = []
            
            for i, sentence_embedding in enumerate(sentence_embeddings):
                similarities = np.dot(source_chunk_embeddings, sentence_embedding) / (
                    np.linalg.norm(source_chunk_embeddings, axis=1) * np.linalg.norm(sentence_embedding)
                )
                max_similarity = np.max(similarities)
                max_similarities.append(max_similarity)
                
                # If similarity is below threshold, it might be a hallucination
                if max_similarity < self.config.get("similarity_threshold", 0.5):
                    unsupported_claims.append(filtered_sentences[i])
            
            # Calculate overall semantic consistency score
            semantic_consistency_score = np.mean(max_similarities) if max_similarities else 0.5
            
            # Higher semantic consistency means lower hallucination score
            hallucination_score = 1.0 - semantic_consistency_score
            
            return {
                "semantic_consistency_score": semantic_consistency_score,
                "hallucination_score": hallucination_score,
                "unsupported_claims": unsupported_claims
            }
            
        except Exception as e:
            logger.error(f"Error in semantic hallucination detection: {str(e)}")
            return {
                "semantic_consistency_score": 0.5,  # Neutral score
                "hallucination_score": 0.5,  # Neutral score
                "error": str(e)
            }

    def _factual_hallucination_detection(
        self, 
        generated_text: str, 
        source_documents: List[Dict[str, Any]], 
        query: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Detect factual inconsistencies between generated text and source documents.
        
        Args:
            generated_text: The text generated by the LLM
            source_documents: List of source documents used for generation
            query: Optional original query for context
            
        Returns:
            Dictionary with factual consistency metrics
        """
        try:
            # Extract numerical facts, entities, and dates from texts
            generated_facts = self._extract_facts(generated_text)
            source_facts = []
            
            for doc in source_documents:
                source_facts.extend(self._extract_facts(doc.get("text", "")))
            
            # Compare facts between generated text and sources
            matching_facts = 0
            non_matching_facts = 0
            
            for gen_fact in generated_facts:
                found_match = False
                
                for src_fact in source_facts:
                    # If facts are the same type and close in value
                    if gen_fact["type"] == src_fact["type"] and self._compare_fact_values(
                        gen_fact["value"], src_fact["value"], fact_type=gen_fact["type"]
                    ):
                        matching_facts += 1
                        found_match = True
                        break
                        
                if not found_match and gen_fact["type"] != "general":
                    non_matching_facts += 1
            
            # Calculate factual consistency score
            total_facts = matching_facts + non_matching_facts
            factual_consistency_score = matching_facts / total_facts if total_facts > 0 else 1.0
            
            # Check citation accuracy if citations are present
            citation_accuracy, incorrect_citations = self._check_citation_accuracy(
                generated_text, source_documents
            )
            
            # Calculate overall factual hallucination score (higher means more hallucination)
            factual_hallucination_score = 1.0 - (
                factual_consistency_score * 0.7 + (citation_accuracy / 100) * 0.3
            )
            
            return {
                "factual_consistency_score": factual_consistency_score,
                "hallucination_score": factual_hallucination_score,
                "citation_accuracy": citation_accuracy,
                "incorrect_citations": incorrect_citations,
                "matching_facts": matching_facts,
                "non_matching_facts": non_matching_facts
            }
            
        except Exception as e:
            logger.error(f"Error in factual hallucination detection: {str(e)}")
            return {
                "factual_consistency_score": 0.5,  # Neutral score
                "hallucination_score": 0.5,  # Neutral score
                "citation_accuracy": 50.0,  # Neutral score
                "error": str(e)
            }

    def _llm_based_verification(
        self, 
        generated_text: str, 
        source_documents: List[Dict[str, Any]], 
        query: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Use LLM to verify if generated content is supported by source documents.
        
        Args:
            generated_text: The text generated by the LLM
            source_documents: List of source documents used for generation
            query: Optional original query for context
            
        Returns:
            Dictionary with LLM verification results
        """
        # Mock implementation for now - would connect to an actual LLM
        # In a production implementation, this would call to a verification LLM
        
        # This is a placeholder for the actual implementation
        return {
            "hallucination_score": 0.1,  # Low score means minimal hallucinations found
            "unsupported_claims": [],
            "verification_confidence": 0.85,
        }
        
    def _extract_sentences(self, text: str) -> List[str]:
        """Extract sentences from text."""
        # Simple sentence splitting - in production would use NLP library
        sentences = re.split(r'(?<=[.!?])\s+', text)
        return [s.strip() for s in sentences if s.strip()]
    
    def _chunk_text(self, text: str, chunk_size: int) -> List[str]:
        """Split text into chunks of specified size."""
        words = text.split()
        chunks = []
        
        for i in range(0, len(words), chunk_size):
            chunk = " ".join(words[i:i + chunk_size])
            chunks.append(chunk)
            
        return chunks
    
    def _extract_facts(self, text: str) -> List[Dict[str, Any]]:
        """Extract facts (numerical values, entities, dates) from text."""
        facts = []
        
        # Extract numerical values with units
        num_pattern = r'(\d+(?:\.\d+)?)\s*([a-zA-Z%]+)?'
        for match in re.finditer(num_pattern, text):
            value = float(match.group(1))
            unit = match.group(2) or ""
            facts.append({
                "type": "numerical",
                "value": value,
                "unit": unit,
                "text": match.group(0)
            })
            
        # Extract dates (simple pattern)
        date_pattern = r'\b(?:\d{1,2}[-/]\d{1,2}[-/]\d{2,4}|\d{4}[-/]\d{1,2}[-/]\d{1,2})\b'
        for match in re.finditer(date_pattern, text):
            facts.append({
                "type": "date",
                "value": match.group(0),
                "text": match.group(0)
            })
            
        # In a real implementation, we would also extract:
        # - Named entities (people, organizations, locations)
        # - Relationships between entities
        # - Claims and statements
        
        return facts
    
    def _compare_fact_values(self, val1: Any, val2: Any, fact_type: str) -> bool:
        """Compare two fact values based on their type."""
        if fact_type == "numerical":
            # Allow for some numerical difference
            return abs(float(val1) - float(val2)) < 0.1 * max(float(val1), float(val2))
        elif fact_type == "date":
            # Simple string comparison for dates
            return val1 == val2
        else:
            # Simple string comparison for other types
            return str(val1).lower() == str(val2).lower()
    
    def _check_citation_accuracy(
        self, 
        generated_text: str, 
        source_documents: List[Dict[str, Any]]
    ) -> Tuple[float, List[Dict[str, Any]]]:
        """Check accuracy of citations in generated text."""
        # Citation patterns: [1], [2], etc.
        citation_pattern = r'\[(\d+)\]'
        citations = re.findall(citation_pattern, generated_text)
        
        # If no citations found, can't measure accuracy
        if not citations:
            return 100.0, []
            
        # Count citations and check if they're in range
        unique_citations = set(int(c) for c in citations)
        valid_citation_range = range(1, len(source_documents) + 1)
        invalid_citations = [c for c in unique_citations if c not in valid_citation_range]
        
        # Calculate accuracy
        if not unique_citations:
            return 100.0, []
            
        accuracy = 100.0 * (len(unique_citations) - len(invalid_citations)) / len(unique_citations)
        
        # Collect incorrect citations
        incorrect = [{"citation": f"[{c}]", "error": "Out of range"} for c in invalid_citations]
        
        return accuracy, incorrect
